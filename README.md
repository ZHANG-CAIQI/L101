This repository is largely based on SF-QA [original repo](https://github.com/soco-ai/SF-QA).

 SF-QA helps you to evaluate your open-domain QA without building the entire open-domain QA pipeline. It supports:
 - Efficient Reader Comparison
 - Reproducible Research 
 - Knowledge Source for Applications

# Features
✨ **Easy evaluation framework:** Build especially for open-domain QA  
✨ **Pre-trained Wiki dataset:** No need to train it yourself   
✨ **Scaleable:** set your own configurations and evaluate on the open domain scale.  
✨ **Open source:** Everyone can contribute

# Relevant repositories:
- DrQA [[codes](https://github.com/facebookresearch/DrQA)] [[paper](https://arxiv.org/pdf/1704.00051.pdf)]
- BERTserini [[codes](https://github.com/castorini/bertserini)] [[paper](https://aclanthology.org/N19-4013.pdf)]
- DS-QA [[paper](https://aclanthology.org/P18-1161.pdf)] [[codes](https://github.com/thunlp/OpenQA)]
- Paragraph Ranker [[codes](https://github.com/jhyuklee/ParagraphRanker)]
- Multi-passage BERT [[codes](https://github.com/soco-ai/SF-QA)] [[paper](https://arxiv.org/pdf/1908.08167.pdf)]
- Answer re-ranker [[codes](https://github.com/shuohangwang/mprc)]
- SPARTA [[codes](https://github.com/soco-ai/SF-QA)] [[paper](https://arxiv.org/pdf/2009.13013.pdf)]
- FID [[paper](https://arxiv.org/pdf/2007.01282.pdf)] [[codes](https://github.com/facebookresearch/FiD)]

Notes: 
- I will (try to) sort out the full version of codes after the release of the final grade.
